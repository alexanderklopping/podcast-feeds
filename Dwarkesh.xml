<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Dwarkesh - Podcast Artikelen</title>
    <description>Artikelen gegenereerd van Dwarkesh podcast afleveringen</description>
    <language>nl</language>
    <lastBuildDate>Wed, 14 Jan 2026 13:08:22 +0000</lastBuildDate>
    <item>
      <title>De Illusie van de Intelligentie-explosie</title>
      <description>In de wereld van kunstmatige intelligentie botsen twee wereldbeelden hard op elkaar. Aan de ene kant staan de korte-termijn optimisten die beweren dat we binnen enkele jaren een intelligentie-explosie zullen meemaken. Aan de andere kant bevinden zich de sceptici die wijzen op de hardnekkige kloof...</description>
      <pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">2026-01-13_Dwarkesh_An audio version of my blog post_ Thoughts on AI progress _Dec 2025_</guid>
      <content:encoded><![CDATA[<h1>De Illusie van de Intelligentie-explosie</h1>
<p>In de wereld van kunstmatige intelligentie botsen twee wereldbeelden hard op elkaar. Aan de ene kant staan de korte-termijn optimisten die beweren dat we binnen enkele jaren een intelligentie-explosie zullen meemaken. Aan de andere kant bevinden zich de sceptici die wijzen op de hardnekkige kloof tussen indrukwekkende demos en echte economische impact. Deze spanning, zo blijkt uit een verhelderende analyse van AI-onderzoeker Dwarkesh Patel, onthult fundamentele misverstanden over wat intelligentie werkelijk betekent en hoe moeilijk het is om menselijke vaardigheden te automatiseren.</p>
<p>---</p>
<p>De paradox begint met een simpele vraag: als we zo dicht bij mensachtige intelligentie staan, waarom investeren de grote AI-laboratoria dan miljarden in het vooraf inbakken van vaardigheden?</p>
<p>"Ik begrijp niet waarom sommige mensen superkorte tijdlijnen hebben, maar tegelijkertijd bullish zijn over het opschalen van reinforcement learning bovenop grote taalmodellen," observeert Patel. "Als we werkelijk dicht bij een mensachtige leerling staan, dan is deze hele aanpak van trainen op verifieerbare uitkomsten gedoemd te mislukken."</p>
<p>De realiteit is dat laboratoria als OpenAI, Google DeepMind en Anthropic momenteel bezig zijn met wat Patel beschrijft als een industriële operatie van vaardighedeninbakking. Er bestaat een complete toeleveringsketen van bedrijven die RL-omgevingen bouwen om modellen te leren navigeren in webbrowsers, Excel-sheets te gebruiken voor financiële modellen, en duizenden andere specifieke taken uit te voeren.</p>
<p>Deze aanpak onthult een fundamentele tegenstelling. Mensen hoeven geen speciale trainingsfase door waarin ze elke softwaretoepassing repeteren die ze ooit nodig zouden kunnen hebben. Een kind kan binnen enkele uren leren om een nieuwe app te gebruiken. Toch vereisen de meest geavanceerde AI-systemen uitgebreide, dure trainingsprocessen voor elke nieuwe vaardigheid.</p>
<p>Baron Millich, een prominente AI-onderzoeker, vatte het probleem scherp samen: "Wanneer we zien dat frontiermodellen verbeteren op verschillende benchmarks, moeten we niet alleen denken aan de toegenomen schaal en slimme ML-onderzoeksideeën, maar ook aan de miljarden dollars die worden betaald aan PhD's, MD's en andere experts om vragen te schrijven en voorbeeldantwoorden en redeneringen te geven die precies op deze vaardigheden zijn gericht."</p>
<p>De spanning wordt het duidelijkst zichtbaar in de robotica. In fundamentele zin is robotica een algoritmeprobleem, geen hardware- of dataprobleem. Met zeer weinig training kan een mens leren hoe hij huidige hardware moet besturen om nuttig werk te verrichten. Als we werkelijk een mensachtige leerling hadden, zou robotica grotendeels een opgelost probleem zijn. Maar het feit dat we zo'n leerling niet hebben, maakt het noodzakelijk om duizenden verschillende huizen in te gaan en miljoenen keren te oefenen met het oppakken van borden of het vouwen van wasgoed.</p>
<p>---</p>
<p>De voorstanders van korte tijdlijnen hebben een antwoord klaar. Hun redenering gaat als volgt: we moeten al deze primitieve RL-technieken gebruiken om een superhuman AI-onderzoeker te bouwen. Vervolgens kunnen miljoenen kopieën van deze geautomatiseerde Ilya Sutskever uitzoeken hoe robuust en efficiënt leren uit ervaring werkt.</p>
<p>Patel is niet overtuigd. "Dit geeft me de vibes van die oude grap: we verliezen geld op elke verkoop, maar we compenseren dat wel in volume," zegt hij. "Op de een of andere manier gaat deze geautomatiseerde onderzoeker het algoritme voor AGI uitvogelen—een probleem waar mensen al een halve eeuw hun hoofd tegen stukslaan—terwijl hij niet de basale leercapaciteiten heeft die kinderen wel hebben. Ik vind het superonwaarschijnlijk."</p>
<p>Bovendien beschrijft dit niet hoe laboratoria daadwerkelijk reinforcement learning benaderen. Je hoeft geen consultant-vaardigheden voor het maken van PowerPoint-slides in te bakken om een onderzoeker te automatiseren. De acties van de laboratoria wijzen op een wereldbeeld waarin deze modellen slecht zullen blijven presteren bij generalisatie en on-the-job leren, waardoor het noodzakelijk is om vooraf de vaardigheden in te bouwen die economisch nuttig zullen zijn.</p>
<p>Een tegenargument luidt dat het gewoon veel efficiënter is om vaardigheden eenmalig in te bouwen tijdens training, in plaats van telkens opnieuw voor elke gebruiker en elk bedrijf. Dit klopt voor gangbare tools zoals browsers en terminals. Een van de belangrijkste voordelen die AGI's zullen hebben is inderdaad hun grotere capaciteit om kennis tussen kopieën te delen.</p>
<p>Maar mensen onderschatten hoeveel bedrijfs- en contextspecifieke vaardigheden nodig zijn voor de meeste banen. En er is momenteel geen robuuste, efficiënte manier voor AI's om deze vaardigheden op te pikken.</p>
<p>---</p>
<p>Een verhelderende uitwisseling illustreerde dit punt. Patel was onlangs bij een diner met een AI-onderzoeker en een bioloog. De bioloog had lange tijdlijnen, en toen gevraagd werd waarom, antwoordde ze: "Een deel van mijn recente werk in het lab bestaat uit het kijken naar slides en beslissen of het stipje op die slide werkelijk een macrofaag is of alleen maar lijkt op een macrofaag."</p>
<p>De AI-onderzoker reageerde zoals je zou verwachten: "Kijk, beeldclassificatie is een standaard deep learning-probleem. Dit staat precies in het centrum van het soort dingen waar we deze modellen voor kunnen trainen."</p>
<p>"Ik vond dit een zeer interessante uitwisseling," reflecteert Patel, "omdat het een kernverschil illustreerde tussen mij en de mensen die transformatieve economische impact verwachten binnen de komende jaren."</p>
<p>Het punt is dit: menselijke werknemers zijn waardevol juist omdat we niet deze moeizame trainingslussen hoeven te bouwen voor elk klein onderdeel van hun werk. Het is niet productief om een aangepaste trainingspijplijn te bouwen om te identificeren hoe macrofagen eruitzien, gegeven de specifieke manier waarop dit lab slides voorbereidt. En vervolgens weer een trainingslus voor de volgende specifieke microtaak, enzovoorts.</p>
<p>Wat je werkelijk nodig hebt is een AI die kan leren van semantische feedback of van zelfgestuurde ervaring, en vervolgens kan generaliseren zoals een mens doet. Elke dag moet je honderd dingen doen die oordeel, situatiebesef en vaardigheden vereisen die on-the-job worden geleerd. Deze taken verschillen niet alleen tussen verschillende mensen, maar zelfs van dag tot dag voor dezelfde persoon.</p>
<p>Het is niet mogelijk om zelfs maar één baan te automatiseren door alleen een voorgedefinieerde set vaardigheden in te bakken, laat staan alle banen.</p>
<p>---</p>
<p>Sommigen beweren dat AI's niet breder zijn ingezet omdat technologie tijd nodig heeft om te diffunderen. Patel ziet dit als een drogreden. "Ik denk dat mensen deze drogreden gebruiken om te verhullen dat deze modellen simpelweg de capaciteiten missen die nodig zijn voor brede economische waarde."</p>
<p>Als deze modellen werkelijk zoals mensen op een server waren, zouden ze ongelooflijk snel diffunderen. Ze zouden zelfs veel gemakkelijker te integreren en in te werken zijn dan een normale menselijke werknemer. Ze zouden binnen minuten de hele Slack en Drive kunnen doorlezen, en ze zouden onmiddellijk alle vaardigheden kunnen distilleren die je andere AI-werknemers hebben.</p>
<p>Bovendien is de arbeidsmarkt voor mensen een zogenaamde 'lemons market'—het is moeilijk om vooraf te bepalen wie de goede mensen zijn, en het aannemen van iemand die slecht blijkt te zijn is zeer kostbaar. Deze dynamiek zou je niet hoeven te trotseren als je gewoon een nieuwe instantie opstart van een getest AGI-model.</p>
<p>"Om deze redenen verwacht ik dat het veel gemakkelijker zal zijn om AI-arbeid in bedrijven te diffunderen dan het is om een persoon aan te nemen," argumenteert Patel. "En bedrijven nemen voortdurend mensen aan."</p>
<p>Als de capaciteiten werkelijk op AGI-niveau zouden zijn, zouden mensen bereid zijn biljoenen dollars per jaar te besteden aan tokens die deze modellen produceren. Kenniswerkers wereldwijd verdienen cumulatief tientallen biljoenen dollars per jaar aan lonen. De reden dat laboratoria momenteel ordes van grootte onder dit cijfer zitten, is dat de modellen nergens in de buurt komen van de capaciteiten van menselijke kenniswerkers.</p>
<p>---</p>
<p>Critici wijzen erop dat de standaard plotseling is geworden dat laboratoria tientallen biljoenen dollars omzet per jaar moeten maken. Tot voor kort vroegen mensen zich af: kunnen deze modellen redeneren? Hebben deze modellen gezond verstand? Doen ze alleen maar patroonherkenning?</p>
<p>AI-optimisten hebben gelijk om AI-pessimisten te bekritiseren voor het herhaaldelijk verplaatsen van deze doelpalen. Het is gemakkelijk om de vooruitgang te onderschatten die AI het afgelopen decennium heeft gemaakt.</p>
<p>Maar een zekere mate van doelpaalverschuiving is eigenlijk gerechtvaardigd. Als je Patel Gemini 3 had laten zien in 2020, zou hij er zeker van zijn geweest dat het de helft van het kenniswerk kon automatiseren. We blijven oplossen wat we dachten dat de voldoende knelpunten waren voor AGI. We hebben modellen die algemeen begrip hebben, ze hebben few-shot learning, ze hebben redenering, en toch hebben we nog steeds geen AGI.</p>
<p>"Wat is een rationele reactie op het observeren hiervan?" vraagt Patel. "Ik denk dat het volkomen redelijk is om hiernaar te kijken en te zeggen: oh, eigenlijk zit er veel meer aan intelligentie en arbeid dan ik eerder besefte. En hoewel we heel dichtbij zijn, en in veel opzichten hebben overtroffen wat ik eerder zou hebben gedefinieerd als AGI, onthult het feit dat modelbedrijven niet de biljoenen dollars omzet maken die door AGI zouden worden geïmpliceerd duidelijk dat mijn vorige definitie van AGI te eng was."</p>
<p>Deze dynamiek zal zich waarschijnlijk blijven herhalen. Tegen 2030 zullen de laboratoria aanzienlijke vooruitgang hebben geboekt op het gebied van continual learning, en de modellen zullen honderden miljarden dollars omzet per jaar genereren. Maar ze zullen niet al het kenniswerk hebben geautomatiseerd.</p>
<p>"Modellen blijven indrukwekkender worden in het tempo dat de korte-tijdlijn mensen voorspellen, maar nuttiger in het tempo dat de lange-tijdlijn mensen voorspellen," vat Patel samen.</p>
<p>---</p>
<p>Dit brengt een cruciale vraag naar voren: wat schalen we eigenlijk op?</p>
<p>Met pre-training hadden we een extreem schone en algemene trend in verbetering en verlies over meerdere ordes van grootte in compute. Zij het op een machtswet, die zo zwak is als exponentiële groei sterk is.</p>
<p>Maar mensen proberen het prestige dat pre-training scaling heeft—bijna zo voorspelbaar als een natuurwet—te gebruiken om bullish voorspellingen over reinforcement learning van verifieerbare beloningen te rechtvaardigen. Hiervoor hebben we geen rijke, publiek bekende trend.</p>
<p>Wanneer ondernemende onderzoekers proberen de implicaties uit schaarse publieke datapunten samen te stellen, krijgen ze behoorlijk pessimistische resultaten. Toby Board schreef een uitstekende analyse waarin hij slim de verbanden legde tussen verschillende O-serie benchmarks. Dit suggereerde hem dat "we zoiets als een miljoen keer opschaling in totale RL-compute nodig hebben om een boost te geven vergelijkbaar met één GPT-niveau."</p>
<p>---</p>
<p>Er is veel gepraat over de mogelijkheid van een software-only singulariteit, waarbij AI-modellen de code schrijven die een slimmer opvolgsysteem genereert. Of een software-plus-hardware singulariteit, waarbij AI's ook de computerhardware van hun opvolgers verbeteren.</p>
<p>Maar al deze scenario's verwaarlozen wat Patel denkt dat de belangrijkste drijfveer zal zijn van verdere verbeteringen bovenop AGI: continual learning.</p>
<p>"Denk eraan hoe mensen capabeler worden dan wat dan ook. Het komt vooral door ervaring in het relevante domein," legt hij uit.</p>
<p>Baron Millich suggereerde in een gesprek een interessante toekomstvisie: continual learning-agenten die allemaal verschillende banen doen en waarde genereren. Vervolgens brengen ze al hun geleerde lessen terug naar het hive mind-model, dat een soort batch-distillatie doet op al deze agenten. De agenten zelf zouden behoorlijk gespecialiseerd kunnen zijn, met wat Andrej Karpathy de 'cognitieve kern' noemde plus kennis en vaardigheden relevant voor de baan waarvoor ze worden ingezet.</p>
<p>Het oplossen van continual learning zal geen eenmalige prestatie zijn. In plaats daarvan zal het voelen als het oplossen van in-context learning. GPT-3 toonde al in 2020 aan dat in-context learning zeer krachtig kon zijn. De mogelijkheden waren zo opmerkelijk dat de titel van het GPT-3 paper was: "Language Models are Few-Shot Learners."</p>
<p>Maar natuurlijk losten we in-context learning niet op toen GPT-3 uitkwam. Er moet nog steeds veel vooruitgang worden geboekt, van begrip tot contextlengte. Patel verwacht een vergelijkbare progressie met continual learning.</p>
<p>Laboratoria zullen waarschijnlijk volgend jaar iets uitbrengen wat ze continual learning noemen, wat inderdaad zal tellen als vooruitgang richting continual learning. Maar menselijk niveau on-the-job leren kan nog 5-10 jaar duren om uit te werken.</p>
<p>---</p>
<p>Dit verklaart waarom Patel geen runaway winsten verwacht van het eerste model dat continual learning kraakt. Als je volledig opgeloste continual learning uit het niets zou hebben, dan zou het inderdaad game over kunnen zijn, zoals Microsoft-CEO Satya Nadella het uitdrukte toen Patel hem hierover ondervroeg in een podcast.</p>
<p>Maar dat is waarschijnlijk niet wat er gaat gebeuren. In plaats daarvan zal een laboratorium uitvogelen hoe ze initiële tractie op dit probleem kunnen krijgen. Vervolgens zal experimenteren met deze functie duidelijk maken hoe het werd geïmplementeerd. Andere laboratoria zullen de doorbraak snel repliceren en licht verbeteren.</p>
<p>Bovendien heeft Patel een prior dat de concurrentie behoorlijk fel zal blijven tussen al deze modelbedrijven. Dit wordt ondersteund door de observatie dat al deze vermeende vliegwielen—of dat nu gebruikersbetrokkenheid op chat is, synthetische data of wat dan ook—zeer weinig hebben gedaan om de steeds heviger wordende concurrentie tussen modelbedrijven te verminderen.</p>
<p>Elke maand wisselen de grote drie modelbedrijven af op het podium, en de andere concurrenten zijn niet zo ver achter. Er lijkt een kracht te zijn—mogelijk talent-poaching, de roddelmolen, of gewoon normale reverse engineering—die tot nu toe elk weglopen voordeel heeft geneutraliseerd dat een enkel laboratorium misschien had.</p>
<p>---</p>
<p>De fundamentele spanning die Patel blootlegt, gaat over meer dan technische voorspellingen. Het gaat over wat intelligentie werkelijk betekent en hoe complex menselijke expertise is. Terwijl AI-systemen steeds indrukwekkender worden in gecontroleerde omgevingen, blijft de kloof tussen demo's en echte economische impact hardnekkig bestaan.</p>
<p>Deze kloof onthult niet dat AI geen vooruitgang boekt—integendeel, de vooruitgang is spectaculair. Maar het suggereert dat echte, mensachtige intelligentie veel subtieler en complexer is dan we aanvankelijk dachten. Het zijn niet alleen de grote cognitieve vaardigheden zoals redenering en begrip. Het zijn de duizenden kleine aanpassingen, de contextgevoeligheid, de vaardigheid om te leren van minimale feedback—kortom, de veerkracht en wendbaarheid die mensen elke dag demonstreren.</p>
<p>De weddenschap van Patel is dat we deze capaciteiten uiteindelijk zullen cracken. Binnen een decennium of twee verwacht hij "werkelijke, breinachtige intelligenties"—iets dat, zoals hij het uitdrukt, "behoorlijk fucking gek" is.</p>
<p>Maar de weg ernaartoe zal waarschijnlijk vol verrassingen zitten, en langer duren dan de kortste tijdlijn optimisten geloven. En wanneer we er aankomen, zal de impact zo fundamentaal zijn dat onze huidige debatten over timing misschien triviaal zullen lijken.</p>
<p>De echte revolutie begint niet met modellen die menselijke taken kunnen nabootsen. Het begint met modellen die kunnen leren zoals mensen—snel, flexibel, en uit de context van hun dagelijkse ervaring. Tot die tijd leven we in een fascinerende tussenperiode: indrukwekkende demonstraties, gestage vooruitgang, maar nog altijd wachtend op de doorbraak die alles verandert.</p>]]></content:encoded>
    </item>
    <item>
      <title>Het Verloren Geheim van de Hersenen</title>
      <description>Adam Marblestone zat in zijn kantoor bij Convergent Research toen het kwartje viel. Na jaren van onderzoek naar hersenwetenschap, van Google DeepMind tot quantum computing, had hij eindelijk een theorie die alles bij elkaar bracht. Het ging niet om de architectuur van de hersenen—dat hadden AI-on...</description>
      <pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">2026-01-13_Dwarkesh_Adam Marblestone _ AI is missing something fundamental about the brain</guid>
      <content:encoded><![CDATA[<h1>Het Verloren Geheim van de Hersenen</h1>
<em>In een tijd waarin kunstmatige intelligentie de grenzen van het mogelijke lijkt te verleggen, worstelt één fundamentele vraag door de hoofden van de slimste onderzoekers ter wereld: hoe doen menselijke hersenen het eigenlijk? Terwijl we triljoenen parameters en exabytes aan data in onze AI-systemen pompen, leert een peuter nog steeds efficiënter dan onze meest geavanceerde modellen. Het antword op dit raadsel zou wel eens kunnen liggen in een deel van de hersenen dat we jarenlang hebben genegeerd.</em>
<p>---</p>
<p>Adam Marblestone zat in zijn kantoor bij Convergent Research toen het kwartje viel. Na jaren van onderzoek naar hersenwetenschap, van Google DeepMind tot quantum computing, had hij eindelijk een theorie die alles bij elkaar bracht. Het ging niet om de architectuur van de hersenen—dat hadden AI-onderzoekers al lang begrepen. Het ging om iets veel fundamentelers: de beloningsfuncties.</p>
<p>"Het is misschien wel de quadriljoen dollar vraag," zei Marblestone tijdens een gesprek over dit mysterie. "We gooien veel meer data naar deze taalmodellen en ze hebben nog steeds maar een klein deel van de totale capaciteiten die een mens heeft. Dus wat is er aan de hand?"</p>
<p>Zijn antwoord zou de manier waarop we over kunstmatige intelligentie denken compleet kunnen veranderen.</p>
<h2>De Vergeten Helft van de Hersenen</h2>
<p>In de wereld van machine learning zijn onderzoekers geobsedeerd door architecturen. Hoeveel lagen heeft je netwerk? Hoe zijn ze verbonden? Welk leeralgoritme gebruik je? Maar volgens Marblestone missen ze het belangrijkste stuk van de puzzel.</p>
<p>"Het veld heeft de rol van zeer specifieke verliesfuncties, zeer specifieke kostenfuncties verwaarloosd," legde hij uit. "Machine learning houdt van wiskundig eenvoudige verliesfuncties, toch? Voorspel het volgende token. Cross-entropie. Deze simpele computerwetenschapper-verliesfuncties."</p>
<p>De evolutie had echter een heel ander plan. Waar AI-onderzoekers kozen voor elegante wiskundige formules, bouwde de evolutie iets veel complexers: een verzameling van duizenden gespecialiseerde beloningsfuncties, elk afgestemd op specifieke overlevingssituaties.</p>
<p>"Ik denk dat de evolutie veel complexiteit heeft ingebouwd in de verliesfuncties," vervolgde Marblestone. "Veel verschillende verliesfuncties voor verschillende gebieden die op verschillende stadia van de ontwikkeling werden ingeschakeld. Veel Python-code, in feite, die een specifiek curriculum genereert voor wat verschillende delen van de hersenen moeten leren."</p>
<p>Dit was geen wild giswerk. Steven Byrnes, een voormalig fysicus die zich had gespecialiseerd in AI-veiligheid, had jaren besteed aan het synthetiseren van neurologie onderzoek om tot een verbluffende conclusie te komen: de hersenen bestaan uit twee fundamenteel verschillende subsystemen.</p>
<h2>Het Twee-Systeem Brein</h2>
<p>Byrnes' theorie was elegant in zijn eenvoud. De hersenen, zo stelde hij, bestaan uit een leersysteem en een stuursysteem. Het leersysteem—voornamelijk de hersenschors—is waar het leren gebeurt. Dit is het deel dat de wereldmodellen bouwt, patronen herkent, en generaliseert van ervaring.</p>
<p>Maar het stuursysteem is waar de magie echt gebeurt. Dit oudere, primitievere deel van de hersenen bevat alle instinctieve reacties die de evolutie heeft gecodeerd: de neiging om van spinnen te griezelen, de drang om status te zoeken, de behoefte om vriendschappen te sluiten.</p>
<p>"Er zijn delen van de amygdala die bijvoorbeeld de spinreflex monitoren en voorspellen," zei Marblestone. "Dus hoe vind je de neuronen die belangrijk zijn voor sociale status? Nou, je hebt een aantal aangeboren heuristieken van sociale status, bijvoorbeeld, of je hebt een aantal aangeboren heuristieken van vriendelijkheid die het stuursysteem kan gebruiken."</p>
<p>Het briljante van dit systeem is hoe het het probleem van generalisatie oplost. De evolutie hoefde niet te voorzien dat mensen ooit podcasts zouden maken of dat iemand zich zou schamen voor het verkeerd beschrijven van Yann LeCun's energiegebaseerde modellen. In plaats daarvan bouwde het een systeem dat geleerde concepten kon koppelen aan primitieve reacties.</p>
<h2>De Spin op je Rug</h2>
<p>Marblestone gebruikte een perfect voorbeeld om dit uit te leggen. Stel je voor dat iemand tegen je zegt: "Er zit een spin op je rug." Je hebt nooit letterlijk een spin op je rug gehad, maar toch voel je die karakteristieke rilling van ongemak.</p>
<p>"Het stuursysteem heeft zijn eigen primitieve zintuiglijke systemen," legde hij uit. "Er kan een aangeboren reactie zijn. Als ik iets zie dat snel naar mijn lichaam beweegt dat ik er niet eerder zag en het is soort van klein en donker en hoog contrast, dat zou een insect kunnen zijn dat op mijn lichaam krieuwelt. Ik ga schrikken."</p>
<p>Maar hoe generaliseert dit naar het woord 'spin'? Hier komt Byrnes' theorie tot leven. Een deel van je amygdala leert om te voorspellen wanneer je gaat schrikken. Het wordt getraind op alle input die het krijgt—inclusing abstracte concepten zoals woorden.</p>
<p>"Dus wat zijn de neuronen die belangrijk zijn in de hersenschors voor sociale status of voor vriendschap? Het zijn degenen die die aangeboren heuristieken voorspellen voor vriendschap," zei Marblestone. "Dus je traint een voorspeller in de hersenschors en je zegt, welke neuronen maken deel uit van de voorspeller? Dat zijn degenen die belangrijk zijn."</p>
<h2>Het Genoom Mysterie Opgelost</h2>
<p>Deze theorie helpt ook om een van de meest verwarrende aspecten van menselijke intelligentie te verklaren: hoe kan zo'n complex systeem ontstaan uit zo weinig genetische informatie?</p>
<p>Het menselijke genoom is slechts drie gigabyte groot, en maar een fractie daarvan codeert voor de hersenen. Dit heeft AI-onderzoekers lang doen twijfelen aan de analogie tussen evolutie en pre-training. Hoe kan evolutie zoveel hebben 'geleerd' met zo weinig data?</p>
<p>"Als een groot deel van het verhaal dat de sample-efficiëntie verhoogt en het leren helpt, de beloningsfunctie is, de verliesfunctie," zei Marblestone, "en als evolutie die verliesfuncties vond die het leren helpen, dan heeft het eigenlijk wel zin hoe je een intelligentie kunt bouwen met zo weinig informatie."</p>
<p>De beloningsfunctie is letterlijk een regel code. Je zou duizend van zulke regels kunnen hebben en dat neemt niet veel ruimte in beslag. Maar deze compacte specificatie kan leiden tot enorme complexiteit door de generalisatiekracht van het leersysteem uit te buiten.</p>
<p>Recent onderzoek van Fei Chen, Evan McCosko en anderen heeft dit idee ondersteund. Door verschillende hersengebieden te analyseren op celtype-niveau, ontdekten ze iets opmerkelijks: er zijn veel meer diverse, gespecialiseerde celtypes in de stuursysteem-gebieden dan in de leersysteem-gebieden.</p>
<p>"In het leersysteem, de cortex-celtypes, is er genoeg om een leeralgoritme op te bouwen," legde Marblestone uit. "In het stuursysteem zijn er duizenden zeer vreemde cellen, die zouden kunnen zijn zoals die voor de spinschrik-reflex, en die voor 'ik ga zo zout proeven.'"</p>
<h2>Waarom LLMs Mislukken</h2>
<p>Deze theorie heeft verstrekkende gevolgen voor kunstmatige intelligentie. De huidige generatie van grote taalmodellen traint op een enkele, eenvoudige taak: voorspel het volgende token. Ze zijn ongelofelijk goed geworden in deze ene taak, maar missen de rijke verzameling van beloningsfuncties die menselijk leren zo efficiënt maken.</p>
<p>"Machine learning houdt van wiskundig eenvoudige verliesfuncties," merkte Marblestone op. "Voorspel het volgende token. Cross-entropie. Deze eenvoudige computerwetenschapper-verliesfuncties."</p>
<p>Maar de hersenen doen iets veel geavanceerdere: omnidirectionele inferentie. In plaats van alleen maar het volgende token te voorspellen, kan elk gebied van de hersenschors elk patroon in elk deel van zijn input voorspellen gegeven elk ander ontbrekend deel.</p>
<p>"Wat als de hersenschors gewoon van nature zo gemaakt is dat elk gebied van de hersenschors elk patroon kan voorspellen in elke subset van zijn inputs gegeven elke andere ontbrekende subsets?" vroeg Marblestone. "Dat is een beetje meer zoals probabilistische AI."</p>
<p>Deze mogelijkheid suggereert dat de hersenen fundamenteel anders werken dan onze huidige AI-systemen. Waar een taalmodel wordt getraind om een zeer specifieke conditionele waarschijnlijkheid te berekenen, zouden de hersenen kunnen werken als een algemene predictie-engine die kan generaliseren naar elke inferentietaak.</p>
<h2>De Ontbrekende Stukjes</h2>
<p>Ondanks deze inzichten erkende Marblestone dat we nog steeds veel niet weten over hoe de hersenen werken. Het onderscheid tussen model-gebaseerd en model-vrij reinforcement learning in de hersenen blijft onduidelijk. De exacte architectuur van corticale circuits is nog steeds een mysterie. En we hebben nog geen volledig beeld van hoe verschillende hersengebieden samenwerken.</p>
<p>"Alle antwoorden die ik geef op deze vragen, alles wat ik zeg, het is allemaal gewoon soort van directioneel," gaf hij toe. "Dat voel ik vanuit de literatuur in die richting wijst op een zeer brede manier. Wat ik eigenlijk wil doen is gaan en de hele muizenbrein in kaart brengen en dit uitvoerig uitzoeken en neurowetenschap de grondwaarheids-wetenschap maken."</p>
<p>Dit brengt ons bij Marblestone's grotere missie. Als CEO van Convergent Research richt hij zich niet alleen op het begrijpen van de hersenen, maar op het transformeren van de neurowetenschap zelf. Hij gelooft dat het veld nieuwe technologieën en grootschalige investeringen nodig heeft om deze fundamentele vragen te beantwoorden.</p>
<h2>De Connectoom Revolutie</h2>
<p>Een van Marblestone's meest ambitieuze projecten is het in kaart brengen van het volledige connectoom—de complete verdeling van alle neurale verbindingen—van zoogdierhersenen. Het klinkt als sciencefiction, maar de technologie komt er snel aan.</p>
<p>E11 Bio, een van de gefocuste onderzoeksorganisaties die Convergent Research ondersteunt, werkt aan het goedkoper maken van connectomische hersenkartering met orders van grootte. De Wellcome Trust schatte dat het eerste muizenhersenconnectoom meerdere miljarden dollars zou kosten. E11's technologie wil dit terugbrengen tot tientallen miljoenen.</p>
<p>"Als je gewoon naïef schaalt, is een menselijke hersenen nu nog steeds miljarden dollars om gewoon één menselijke hersenen te doen," zei Marblestone. "Maar ik weet niet zeker of je elke neuron in de menselijke hersenen nodig hebt. Ik denk dat we bijvoorbeeld een hele muizenhersenen willen doen en een menselijk stuursysteem."</p>
<p>De technologische doorbraak komt van de overstap van elektronenmicroscopie naar optische microscopie. Met optische technieken kunnen onderzoekers niet alleen zien wie met wie verbonden is, maar ook welke moleculen aanwezig zijn bij elke synaps. Dit geeft een veel rijker beeld van hoe hersencircuits daadwerkelijk functioneren.</p>
<h2>De Toekomst van Intelligentie</h2>
<p>Als Marblestone gelijk heeft, dan kijken we misschien naar de verkeerde plek voor de volgende doorbraak in AI. In plaats van steeds grotere modellen te bouwen met steeds meer data, moeten we misschien begrijpen hoe we betere beloningsfuncties kunnen ontwerpen.</p>
<p>Dit zou kunnen betekenen dat we systemen moeten bouwen die meer lijken op de hersenen: met gespecialiseerde subsystemen voor leren en sturen, met rijke verzamelingen van beloningsfuncties die zijn afgestemd op specifieke taken, en met de mogelijkheid om omnidirectionele inferentie uit te voeren in plaats van alleen het volgende token te voorspellen.</p>
<p>"Mijn persoonlijke intuïtie binnen dat kader is dat het veld de rol van deze zeer specifieke verliesfuncties heeft verwaarloosd," zei Marblestone. "Zeer specifieke kostenfuncties."</p>
<p>De implicaties strekken zich uit tot AI-veiligheid. Als intelligentie inderdaad afhankelijk is van het hebben van de juiste beloningsfuncties, dan wordt de vraag hoe we superintelligente systemen afstemmen op menselijke waarden nog urgenter. We kunnen geen paperclip-maximaliseerders bouwen die toevallig superintelligent zijn—ze zouden de complexe verzameling van sociale instincten missen die menselijke intelligentie sturen.</p>
<h2>Een Nieuwe Wetenschappelijke Revolutie</h2>
<p>Marblestone's visie gaat verder dan alleen het begrijpen van de hersenen of het bouwen van betere AI. Hij stelt voor dat we de manier waarop we wetenschap doen fundamenteel moeten veranderen. Door de neurowetenschap te voorzien van de juiste technologische hulpmiddelen—van connectomen tot formele bewijssystemen—kunnen we eindelijk de diepe vragen beantwoorden die ons al tientallen jaren bezighouden.</p>
<p>"We moeten het veld van de neurowetenschap in staat stellen om gewoon van de neurowetenschap een krachtiger veld te maken, technologisch en anderszins, om daadwerkelijk een vraag als deze te kunnen kraken," zei hij.</p>
<p>Het is een ambitieuze visie, maar gezien de stakes—het begrijpen van intelligentie zelf—misschien wel de moeite waard. Als we ooit echte kunstmatige intelligentie willen bouwen, systemen die niet alleen slim zijn maar ook wijze, dan moeten we eerst begrijpen hoe de natuur dit kunststukje vier miljard jaar geleden voor het eerst wist te flikken.</p>
<p>De antwoorden liggen daar, verborgen in de drie pond grijs weefsel tussen onze oren. We hoeven ze alleen maar te vinden.</p>]]></content:encoded>
    </item>
  </channel>
</rss>